<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yangyangu.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="写在前面 接触机器学习有好几年了，期间干了一阵子的大数据，最近想要捡起来也蛮不容易的。想着在自己的博客上把学到的一些知识记录下来，也算是复习巩固了。 \(Support{\,}Vector{\,}Machine\)（支持向量机）是机器学习中的算法。 线性模型 假定在一个平面内，能够用一条直线将正负样本分开，那么就称为线性可分（\(Liner{\,} Sepratable\)）。否则就是线性不可分。">
<meta property="og:type" content="article">
<meta property="og:title" content="支持向量机SVM——简述原理">
<meta property="og:url" content="http://yangyangu.github.io/2021/11/07/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BASVM%E2%80%94%E2%80%94%E7%AE%80%E8%BF%B0%E5%8E%9F%E7%90%86/index.html">
<meta property="og:site_name" content="小羊的算法笔记">
<meta property="og:description" content="写在前面 接触机器学习有好几年了，期间干了一阵子的大数据，最近想要捡起来也蛮不容易的。想着在自己的博客上把学到的一些知识记录下来，也算是复习巩固了。 \(Support{\,}Vector{\,}Machine\)（支持向量机）是机器学习中的算法。 线性模型 假定在一个平面内，能够用一条直线将正负样本分开，那么就称为线性可分（\(Liner{\,} Sepratable\)）。否则就是线性不可分。">
<meta property="og:locale">
<meta property="article:published_time" content="2021-11-07T13:04:53.000Z">
<meta property="article:modified_time" content="2022-01-11T15:07:57.869Z">
<meta property="article:author" content="一口羊宝wu">
<meta property="article:tag" content="机器学习 手推算法 学习笔记">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://yangyangu.github.io/2021/11/07/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BASVM%E2%80%94%E2%80%94%E7%AE%80%E8%BF%B0%E5%8E%9F%E7%90%86/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-Hans'
  };
</script>

  <title>支持向量机SVM——简述原理 | 小羊的算法笔记</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">小羊的算法笔记</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://yangyangu.github.io/2021/11/07/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BASVM%E2%80%94%E2%80%94%E7%AE%80%E8%BF%B0%E5%8E%9F%E7%90%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="一口羊宝wu">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="小羊的算法笔记">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          支持向量机SVM——简述原理
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-11-07 21:04:53" itemprop="dateCreated datePublished" datetime="2021-11-07T21:04:53+08:00">2021-11-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-01-11 23:07:57" itemprop="dateModified" datetime="2022-01-11T23:07:57+08:00">2022-01-11</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="写在前面">写在前面</h1>
<p>接触机器学习有好几年了，期间干了一阵子的大数据，最近想要捡起来也蛮不容易的。想着在自己的博客上把学到的一些知识记录下来，也算是复习巩固了。</p>
<p><span class="math inline">\(Support{\,}Vector{\,}Machine\)</span>（支持向量机）是机器学习中的算法。</p>
<h1 id="线性模型">线性模型</h1>
<p>假定在一个平面内，能够用一条直线将正负样本分开，那么就称为线性可分（<span class="math inline">\(Liner{\,} Sepratable\)</span>）。否则就是线性不可分。</p>
<h1 id="问题">问题</h1>
<p>显然有很多直线可以区分，那么如何衡量哪条直线更好？</p>
<h1 id="性能指标">性能指标</h1>
<p>1、将直线平移到一类样本的最近，得到L1</p>
<p>2、重复这一个过程移动到另一类样本最近，得到L2</p>
<p>3、计算<span class="math inline">\(d=L1\)</span>到<span class="math inline">\(L2\)</span>的距离</p>
<p>那么d即优化的目标，寻找L1和L2，使得d最大。</p>
<p><span class="math inline">\(d\)</span>: 间隔（<span class="math inline">\(Margin\)</span>）</p>
<p>将平行线碰到的向量称为支持向量（<span class="math inline">\(Support{\,}Vector\)</span>）</p>
<p>实际上，<span class="math inline">\(L\)</span>只与少数支持向量有关，故可以用在小样本上。</p>
<h1 id="数学描述">数学描述</h1>
<p>【参考： 《支持向量机导论》】</p>
<h2 id="定义">定义</h2>
<p>1、训练数据、标签</p>
<p><span class="math inline">\((x_i, y_i), i = 1 \cdots N\)</span></p>
<p><span class="math inline">\(x_i\)</span>为向量， <span class="math inline">\(y_i\)</span>为标签(+1 or -1)。注：这里是一个二分类问题。</p>
<p>2、线形模型：找到一个超平面<span class="math inline">\((\omega，b)\)</span>，满足<span class="math inline">\(\omega^TX + b = 0\)</span></p>
<p><span class="math inline">\(\omega\)</span>也是一个向量，维度与<span class="math inline">\(x_i\)</span>相等</p>
<p>3、一个训练集线性可分是指：</p>
<p>对于训练集{(<span class="math inline">\(x_i, y_i\)</span>)}, <span class="math inline">\(i=1\cdots N\)</span>,</p>
<p><span class="math inline">\(\exists (\omega, b)\)</span>，使得对<span class="math inline">\(\forall i=1\cdots N\)</span>，有:</p>
<p><span class="math inline">\(\left\{\begin{aligned} \omega^Tx_i + b \geq 0,y_i = + 1\\ \omega^Tx_i + b \leq 0,y_i = -1 \end{aligned}\right.\)</span></p>
<p>即：<span class="math inline">\(y_i[\omega^Tx_i + b] \geq 0\)</span></p>
<h2 id="机器学习步骤">机器学习步骤</h2>
<h3 id="step1">Step1</h3>
<p>限定模型。用一个方程。</p>
<h3 id="step2">Step2</h3>
<p>模型留出待定参数。此处为W和b</p>
<h3 id="step3">Step3</h3>
<p>用训练数据确定W和b的值。训练完成。</p>
<h2 id="优化问题凸优化问题二次优化问题">优化问题（凸优化问题/二次优化问题）</h2>
<p>最小化(Minimize): <span class="math inline">\(\frac{1}{2} ||\omega||^2\)</span></p>
<p>限制条件(Subject to): <span class="math inline">\(y_i[\omega^Tx_i + b] \geq 1, i=1\cdots N\)</span></p>
<h2 id="注意到如下两个事实">注意到如下两个事实：</h2>
<h3 id="事实1">事实1:</h3>
<p><span class="math inline">\(\omega^Tx + b = 0\)</span>与<span class="math inline">\(a\omega^Tx + b = 0\)</span>是同一个平面。<span class="math inline">\(a\in R^+\)</span>。</p>
<p>若<span class="math inline">\((\omega, b)\)</span>满足公式一，则<span class="math inline">\((a\omega, ab)\)</span>也满足。</p>
<h3 id="事实2">事实2:</h3>
<p>点到平面距离公式。</p>
<p>平面<span class="math inline">\(\omega_1x + \omega_2y + b = 0\)</span>，则<span class="math inline">\((x_0, y_0)\)</span>到平面距离<span class="math inline">\(d=\frac{|\omega_1x_0 + \omega_2y_0 + b|}{\sqrt{\omega_1^2 | \omega_2^2}}\)</span></p>
<h2 id="推论">推论</h2>
<p>根据事实1，可以用<span class="math inline">\(a\)</span>去放缩<span class="math inline">\((w, b)\longrightarrow(a\omega, ab)\)</span>，最终使得在支持向量<span class="math inline">\(x_0\)</span>上有：</p>
<p><span class="math inline">\(|\omega^Tx_0 + b| = 1\)</span></p>
<p>（也就是说可以通过对<span class="math inline">\((\omega, b)\)</span>进行缩放，使得<span class="math inline">\(d\)</span>的分母为1）</p>
<p>此时支持向量与平面的距离<span class="math inline">\(d=\frac{1}{||\omega||}\)</span>（根据事实2）</p>
<p>故<span class="math inline">\(max(d) = minimize(||\omega||)\)</span></p>
<p>这里将最大化<span class="math inline">\(d\)</span>转化为最小化<span class="math inline">\(\omega\)</span>的模。</p>
<p>接下来是解决优化问题。</p>
<h1 id="二次规划quadratic-programming">二次规划(Quadratic Programming)</h1>
<p>1、目标函数(Object Function) 二次项</p>
<p>2、限制条件。一次项。</p>
<p>要么无解，要么只有一个极值，且为全局最优值。（目标函数为凸函数）</p>
<h1 id="非线性模型">非线性模型</h1>
<h2 id="svm如何处理非线性模型">SVM如何处理非线性模型？</h2>
<p>1、最小化 <span class="math inline">\(\frac{1}{2} ||\omega||^2 + c\sum_{i=1}^N\xi_i\)</span>（式1）</p>
<p>这里的<span class="math inline">\(c\)</span>也是事先设定的参数，<span class="math inline">\(c\sum_{i=1}^N\xi_i\)</span>为正则项(Regulation Term)</p>
<p>2、限制条件 <span class="math inline">\(\left\{\begin{matrix}y_i[\omega^Tx_i + b] \geq 1 - \xi_i (i=1\cdots N) \\ \xi_i \geq 0 \end{matrix}\right.\)</span></p>
<p>3、定义高维映射<span class="math inline">\(\varphi(x)\)</span></p>
<p>若<span class="math inline">\(x\)</span>为低维、不可分的数据，经过<span class="math inline">\(x \rightarrow \varphi(x)\)</span>， <span class="math inline">\(\varphi(x)\)</span>为高维数据，那么<span class="math inline">\(\varphi(x)\)</span>可分的概率更高。</p>
<p>根据<span class="math inline">\(x_i = \varphi(x)\)</span>，此时限制条件可以修改为<span class="math inline">\(y_i[\omega^T\varphi(x) + b] &gt;= 1 - \xi_i\)</span></p>
<h2 id="如何选出varphix">如何选出<span class="math inline">\(\varphi(x)\)</span>？</h2>
<p>首先，<span class="math inline">\(\varphi(x)\)</span>是无限维。那么此时<span class="math inline">\(\varphi(x)\)</span>线性可分的概率为1。</p>
<p>其次，我们可以不知道无限维<span class="math inline">\(\varphi(x)\)</span>的显式表达式，我们只需知道一个核函数(Kernel function):</p>
<p><span class="math inline">\(K(x_1, x_2) = \varphi(x_1)^T\varphi(x_2)\)</span></p>
<p>则式1的优化式仍然可解。</p>
<h2 id="核函数">核函数</h2>
<p>1、高斯核：<span class="math inline">\(K(x_1, x_2) = e^-\frac{||x_1 - x_2||^2}{2\sigma^2}\)</span> ，<span class="math inline">\(\varphi(x)\)</span>为无限维。</p>
<p>2、多项式核：<span class="math inline">\(K(x_1, x_2) = (x_1^Tx_2 + 1)^d， d\)</span>为 多项式阶数，<span class="math inline">\(\varphi(x)\)</span>为有限维。</p>
<p>如何在只知道<span class="math inline">\(K(x_1, x_2)\)</span>、不知道<span class="math inline">\(\varphi(x)\)</span>的情况下去替换？</p>
<h2 id="优化理论">优化理论</h2>
<h3 id="原问题prime-problem">原问题(Prime Problem)：</h3>
<p>这是一个非常general的定义。</p>
<p>最小化<span class="math inline">\(f(\omega)\)</span></p>
<p>限制条件: <span class="math inline">\(\left\{\begin{matrix}g_i(\omega) \leq 0, i=1\cdots K (K个不等式定义) \\ h_i(\omega) = 0，i=1\cdots M (M个不等式定义)\end{matrix}\right.\)</span></p>
<hr />
<h3 id="对偶问题dual-problem">对偶问题(Dual Problem):</h3>
<p>1、定义：</p>
<p><span class="math inline">\(L(\omega, \alpha, \beta) = f(\omega) + \sum_{i=1}^K\alpha_ig_i(\omega)+\sum_{i=1}^M\beta_ih_i(\omega) \\=f(\omega) + \alpha^Tg(\omega) + \beta^Th(\omega)\)</span> (向量形式)</p>
<p>2、对偶问题定义：</p>
<p>最大化：<span class="math inline">\(\theta(\alpha, \beta) = inf\left\{\begin{matrix}L(\omega, \alpha,\beta)\end{matrix}\right\}\)</span></p>
<p><span class="math inline">\(inf\)</span>为求最小值，限制<span class="math inline">\(\alpha和\beta\)</span>，遍历所有<span class="math inline">\(\omega\)</span>，使得<span class="math inline">\(L\)</span>最小，取最小的<span class="math inline">\(L\)</span>时的<span class="math inline">\(\omega\)</span>。</p>
<p>限制条件： <span class="math inline">\(\alpha_i \geq 0(i=1\cdots K)\)</span> 或 <span class="math inline">\(\alpha \geq 0\)</span>（向量的每一个分量都大于或等于0）</p>
<p>定义：<span class="math inline">\(G=f(\omega^*) - \theta(\alpha^*, \beta^*) \geq 0\)</span>，<span class="math inline">\(G\)</span>为原问题与对偶问题的间距(Duality Gap)。 （对于某些问题，可以证明<span class="math inline">\(G=0\)</span>）</p>
<h3 id="强对偶定理">强对偶定理</h3>
<p>这里不加证明地给出强对偶定理。</p>
<p>若<span class="math inline">\(f(\omega)\)</span>为凸函数，且<span class="math inline">\(g(\omega)=A\omega+b, h(\omega) = c\omega + d\)</span>，则此优化问题的原问题与对偶问题的<span class="math inline">\(G=0\)</span>。</p>
<p>即：<span class="math inline">\(f(\omega^x) = \theta(\alpha^x, \beta^*)\)</span>（意味着上面的证明中有两处需要取等号）</p>
<p>对 <span class="math inline">\(\forall i = 1\cdots K\)</span>(KKT条件),</p>
<p><span class="math inline">\(\left\{\begin{matrix}\ 或者 \alpha^*_i = 0 \\ 或者 g_i^*(\omega^*) = 0\end{matrix}\right.\)</span></p>
<h3 id="引入svm非线性模型">引入SVM非线性模型</h3>
<h4 id="优化目标">优化目标</h4>
<p>最小化 <span class="math inline">\(\frac{1}{2}||\omega||^2 + c\sum_{i=1}^N\xi_i\)</span>（首先这是一个凸函数）</p>
<p>限制条件：<span class="math inline">\(\left\{\begin{matrix}\ y_i[\omega^T\varphi(x)+b] \geq 1 - \xi_i \\ \xi_i \geq 0\end{matrix}\right.\)</span></p>
<h4 id="标准形式">标准形式</h4>
<p><span class="math inline">\(minimize: \frac{1}{2}||\omega||^2 - c\sum_{i=1}^N\xi_i\)</span></p>
<p><span class="math inline">\(subject{\,}to :\left\{\begin{matrix}\ 1+\xi_i-y_i\omega^T\varphi(x_i) - y_ib \leq 0 \\ \xi_i \leq 0 (i=1\cdots N)\end{matrix}\right.\)</span></p>
<h4 id="对偶问题">对偶问题</h4>
<p><span class="math inline">\(maximum: \theta(\alpha, \beta) = inf\left\{\begin{matrix}\frac{1}{2}||\omega||^2-c\sum_{i=1}^N\xi_i + \sum_{i=1}^N\beta_i\xi_i + \sum_{i=1}^N\alpha_i[1+\xi_i-y_i\omega^T\varphi(x_i)-y_ib]\end{matrix}\right\}\)</span>,对所有的<span class="math inline">\((\omega_i, \xi_i, \beta_i)\)</span></p>
<h4 id="补充">补充</h4>
<p>1、<span class="math inline">\(\omega\)</span>为一个向量，<span class="math inline">\(\omega=(\omega_1, \omega_2, \cdots, \omega_m)^T, f(\omega)\)</span>为一个数</p>
<p><span class="math inline">\(\frac{\partial f}{\partial \omega} = (\frac{\partial f}{\partial \omega_1}, \frac{\partial f}{\partial \omega_2}, \cdots, \frac{\partial f}{\partial \omega_m})^T\)</span></p>
<p>2、若<span class="math inline">\(f(\omega)=\frac{1}{2}||\omega||^2\)</span>， 则<span class="math inline">\(\frac{\partial f}{\partial \omega} = \omega\)</span></p>
<p>3、若<span class="math inline">\(f(\omega)=\omega^Tx\)</span>, 则<span class="math inline">\(\frac{\partial f}{\partial \omega} = x\)</span></p>
<h4 id="求解">求解</h4>
<p><span class="math inline">\(\frac{\partial L}{\partial \omega} = 0 \Rightarrow \omega-\sum_{i=1}^N\alpha_iy_i\varphi(x_i) = 0(用补充1) \Rightarrow \omega = \sum_{i=1}^N\alpha_iy_i\varphi(x_i)\)</span></p>
<p><span class="math inline">\(\frac{\partial L}{\partial \xi_i} = 0 \Rightarrow -c+\beta_i + \alpha_i = 0 \Rightarrow \alpha_i + \beta_i = c\)</span></p>
<p><span class="math inline">\(\frac{\partial L}{\partial b} = 0 \Rightarrow -\sum_{i=1}^N\alpha_iy_i=0 \Rightarrow \sum_{i=1}^N\alpha_iy_i = 0\)</span></p>
<p>代入<span class="math inline">\(\theta(\alpha, \beta) = L(\omega_i,\xi_i,b)\)</span>中，</p>
<p>化简： <span class="math inline">\(\theta(\alpha, \beta) = \sum_{i=1}^N\alpha_i-\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_jK(x_i, x_j)\)</span></p>
<p>由<span class="math inline">\(\frac{1}{2}||\omega||^2 = \frac{1}{2}\omega^T\omega\)</span></p>
<p><span class="math inline">\(= \frac{1}{2}(\sum_{i=1}^N\alpha_iy_i\varphi(x_i))^T(\sum_{j=1}^N\alpha_jy_j\varphi(x_j))\)</span></p>
<p><span class="math inline">\(= \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j\varphi(x_i)^T\varphi(x_j)\)</span></p>
<p><span class="math inline">\(=\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_jK(x_i, x_j)\)</span></p>
<p>这里用到了 <span class="math inline">\(K(x_i, x_j)=\varphi(x_i)^T\varphi(x_j)\)</span></p>
<p>注意到<span class="math inline">\(K(x_i, x_j)=K(x_j, x_i)\)</span></p>
<p><span class="math inline">\(- \sum_{i=1}^N\alpha_iy_i\omega^T\varphi(x_i)\)</span></p>
<p><span class="math inline">\(= -\sum_{i=1}^N\alpha_Iy_i(\sum_{j=1}^N\alpha_jy_j\varphi(x_j))^T\varphi(x_i)\)</span></p>
<p><span class="math inline">\(= -\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j\varphi(x_i)^T\varphi(x_j)\)</span></p>
<p><span class="math inline">\(= -\sum_{j=1}^N\alpha_i\alpha_jy_iy_jK(x_i, x_j)\)</span></p>
<h3 id="结论">结论</h3>
<p>最大化：<span class="math inline">\(\theta(\alpha, \beta) = \sum_{i=1}^N\alpha_i - \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_jK(x_i, x_j)\)</span></p>
<p>限制条件：<span class="math inline">\(\left\{\begin{matrix}\ 0 \leq \alpha_i \leq c \\ \sum_{i=1}^N\alpha_iy_i=0 \end{matrix}\right.\)</span></p>
<p><span class="math inline">\(KKT\)</span>条件：<span class="math inline">\(\forall i=1 \cdots K, \alpha_i^*=0或g_i^*(\omega^*) = 0\)</span></p>
<p>解法：<span class="math inline">\(SMO\)</span>算法</p>
<h3 id="参数求解">参数求解</h3>
<p>对于测试样本<span class="math inline">\(x\)</span>，<span class="math inline">\(\left\{\begin{matrix}\ 若\omega^T\varphi(x) + b \geq 0, 则y=+1 \\ 若\omega^T\varphi(x) + b &lt; 0, 则y=-1 \end{matrix}\right.\)</span></p>
<p>根据<span class="math inline">\(\omega^T\varphi(x) = \sum_{i=1}^N[\alpha_iy_i\varphi(x_i)]^T\varphi(x) \\ = \sum_{i=1}^N\alpha_iy_i[\varphi(x_i)]^T\varphi(x) \\ = \sum_{i=1}^N\alpha_iy_iK(x_i, x)\)</span></p>
<p>可知，不需要知道参数<span class="math inline">\(\omega\)</span>，只需要知道<span class="math inline">\(\alpha_i和K(x, y)\)</span></p>
<p>下面求解<span class="math inline">\(b\)</span>，根据<span class="math inline">\(KKT\)</span>条件，</p>
<p><span class="math inline">\(\forall i = 1 \cdots N\)</span>，<span class="math inline">\(\left\{\begin{matrix}\ 要么\beta_i=0，要么\xi_i=0 \\ 要么\alpha_i=0，要么1+\xi_i - y_i\omega^T\varphi(x_i) - y_ib = 0 \end{matrix}\right.\)</span></p>
<p>取一个<span class="math inline">\(0&lt;\alpha_i &lt; c \Rightarrow\)</span> <span class="math inline">\(\left\{\begin{matrix}\ \alpha_i \neq 0 \\ \beta_i = c - \alpha_I &gt; 0 \end{matrix}\right.\)</span> <span class="math inline">\(\Rightarrow \beta_i \neq 0 \Rightarrow \xi_i = 0\)</span></p>
<p><span class="math inline">\(\alpha_i \neq 0 \Rightarrow 1 - y_i\omega^T\varphi(x_i) - y_ib = 0 \\ \Rightarrow b = \frac{1 - y_i\omega^T\varphi(x_i)}{y_i} = \frac{1 - y_i\sum_{j = 1}^N\alpha_jy_jK(x_i, x_j)}{y_i}\)</span></p>
<h3 id="参考资料">参考资料：</h3>
<p>1、《Convex Optimization》 Stephen Boyd著，即《凸优化》</p>
<p>2、《Nonliner Programming》</p>
<h1 id="svm算法">SVM算法</h1>
<h2 id="训练流程">训练流程</h2>
<p>输入<span class="math inline">\((x_i, y_i), i= 1 \cdots N\)</span></p>
<p>解优化问题：</p>
<p>最大化：<span class="math inline">\(\theta(\alpha) = \sum_{i=1}^N\alpha_i - \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_jK(x_i, x_j)\)</span></p>
<p>限制条件：<span class="math inline">\(\left\{\begin{matrix}\ 0 \leq \alpha_i \leq c \\ \sum_{i=1}^N\alpha_iy_i=0 \end{matrix}\right.\)</span></p>
<p>使用<span class="math inline">\(SMO\)</span>算法求解出<span class="math inline">\(\alpha_i\)</span></p>
<p>求解<span class="math inline">\(b\)</span>：找一个<span class="math inline">\(0 &lt; \alpha_i &lt; c\)</span>，<span class="math inline">\(b = \frac{1 - y_i\sum_{j=1}^N\alpha_jy_jK(x_i, x_j)}{y_i}\)</span></p>
<h2 id="测试流程">测试流程</h2>
<p>输入测试样本<span class="math inline">\(x\)</span></p>
<p><span class="math inline">\(\left\{\begin{matrix}\ 若\sum_{i=1}^N\alpha_iy_iK(x_i, x) \geq 0, 则y=+1 \\ 若\sum_{i=1}^N\alpha_iy_iK(x_i, x) &lt; 0 则y=-1\end{matrix}\right.\)</span></p>
<h1 id="svm-核函数">SVM 核函数</h1>
<p><span class="math inline">\(Linear\)</span>（线性内核）：<span class="math inline">\(K(x, y) = x^Ty\)</span>（等于不用核函数）</p>
<p><span class="math inline">\(Poly\)</span>（多项式核）：<span class="math inline">\(K(x, y) = (x^Ty + 1)^d\)</span>（可以写出<span class="math inline">\(\varphi(x)\)</span>）</p>
<p><span class="math inline">\(Rbf\)</span>（高斯径向基函数核）：<span class="math inline">\(K(x, y) = e^-\frac{||x-y||^2}{\sigma^2}\)</span></p>
<p><span class="math inline">\(Tanh核\)</span>：<span class="math inline">\(K(x, y) = tanh(\beta x^Ty + b), tanh=\frac{e^x - e^-x}{e^x + e^-x}\)</span></p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%89%8B%E6%8E%A8%E7%AE%97%E6%B3%95-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" rel="tag"># 机器学习 手推算法 学习笔记</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/03/09/%E5%9B%BE%E8%AE%BA%E7%9B%B8%E5%85%B3-%E7%AE%97%E6%B3%95%E6%A8%A1%E6%9D%BF/" rel="prev" title="图论相关--算法模板">
      <i class="fa fa-chevron-left"></i> 图论相关--算法模板
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/11/11/%E5%A6%82%E4%BD%95%E5%9C%A8hexo%E5%8D%9A%E5%AE%A2%E4%B8%AD%E5%86%99Latex%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F/" rel="next" title="如何在hexo博客中写Latex数学公式">
      如何在hexo博客中写Latex数学公式 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%86%99%E5%9C%A8%E5%89%8D%E9%9D%A2"><span class="nav-number">1.</span> <span class="nav-text">写在前面</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B"><span class="nav-number">2.</span> <span class="nav-text">线性模型</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%97%AE%E9%A2%98"><span class="nav-number">3.</span> <span class="nav-text">问题</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87"><span class="nav-number">4.</span> <span class="nav-text">性能指标</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%95%B0%E5%AD%A6%E6%8F%8F%E8%BF%B0"><span class="nav-number">5.</span> <span class="nav-text">数学描述</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9A%E4%B9%89"><span class="nav-number">5.1.</span> <span class="nav-text">定义</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%AD%A5%E9%AA%A4"><span class="nav-number">5.2.</span> <span class="nav-text">机器学习步骤</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#step1"><span class="nav-number">5.2.1.</span> <span class="nav-text">Step1</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#step2"><span class="nav-number">5.2.2.</span> <span class="nav-text">Step2</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#step3"><span class="nav-number">5.2.3.</span> <span class="nav-text">Step3</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98%E5%87%B8%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98%E4%BA%8C%E6%AC%A1%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98"><span class="nav-number">5.3.</span> <span class="nav-text">优化问题（凸优化问题&#x2F;二次优化问题）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B3%A8%E6%84%8F%E5%88%B0%E5%A6%82%E4%B8%8B%E4%B8%A4%E4%B8%AA%E4%BA%8B%E5%AE%9E"><span class="nav-number">5.4.</span> <span class="nav-text">注意到如下两个事实：</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BA%8B%E5%AE%9E1"><span class="nav-number">5.4.1.</span> <span class="nav-text">事实1:</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BA%8B%E5%AE%9E2"><span class="nav-number">5.4.2.</span> <span class="nav-text">事实2:</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8E%A8%E8%AE%BA"><span class="nav-number">5.5.</span> <span class="nav-text">推论</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BA%8C%E6%AC%A1%E8%A7%84%E5%88%92quadratic-programming"><span class="nav-number">6.</span> <span class="nav-text">二次规划(Quadratic Programming)</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B"><span class="nav-number">7.</span> <span class="nav-text">非线性模型</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#svm%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B"><span class="nav-number">7.1.</span> <span class="nav-text">SVM如何处理非线性模型？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E9%80%89%E5%87%BAvarphix"><span class="nav-number">7.2.</span> <span class="nav-text">如何选出\(\varphi(x)\)？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A0%B8%E5%87%BD%E6%95%B0"><span class="nav-number">7.3.</span> <span class="nav-text">核函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA"><span class="nav-number">7.4.</span> <span class="nav-text">优化理论</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8E%9F%E9%97%AE%E9%A2%98prime-problem"><span class="nav-number">7.4.1.</span> <span class="nav-text">原问题(Prime Problem)：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AF%B9%E5%81%B6%E9%97%AE%E9%A2%98dual-problem"><span class="nav-number">7.4.2.</span> <span class="nav-text">对偶问题(Dual Problem):</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BC%BA%E5%AF%B9%E5%81%B6%E5%AE%9A%E7%90%86"><span class="nav-number">7.4.3.</span> <span class="nav-text">强对偶定理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BC%95%E5%85%A5svm%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B"><span class="nav-number">7.4.4.</span> <span class="nav-text">引入SVM非线性模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E7%9B%AE%E6%A0%87"><span class="nav-number">7.4.4.1.</span> <span class="nav-text">优化目标</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A0%87%E5%87%86%E5%BD%A2%E5%BC%8F"><span class="nav-number">7.4.4.2.</span> <span class="nav-text">标准形式</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AF%B9%E5%81%B6%E9%97%AE%E9%A2%98"><span class="nav-number">7.4.4.3.</span> <span class="nav-text">对偶问题</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%A1%A5%E5%85%85"><span class="nav-number">7.4.4.4.</span> <span class="nav-text">补充</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B1%82%E8%A7%A3"><span class="nav-number">7.4.4.5.</span> <span class="nav-text">求解</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BB%93%E8%AE%BA"><span class="nav-number">7.4.5.</span> <span class="nav-text">结论</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%82%E6%95%B0%E6%B1%82%E8%A7%A3"><span class="nav-number">7.4.6.</span> <span class="nav-text">参数求解</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="nav-number">7.4.7.</span> <span class="nav-text">参考资料：</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#svm%E7%AE%97%E6%B3%95"><span class="nav-number">8.</span> <span class="nav-text">SVM算法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E6%B5%81%E7%A8%8B"><span class="nav-number">8.1.</span> <span class="nav-text">训练流程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B5%8B%E8%AF%95%E6%B5%81%E7%A8%8B"><span class="nav-number">8.2.</span> <span class="nav-text">测试流程</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#svm-%E6%A0%B8%E5%87%BD%E6%95%B0"><span class="nav-number">9.</span> <span class="nav-text">SVM 核函数</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">一口羊宝wu</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">48</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">21</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">一口羊宝wu</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
